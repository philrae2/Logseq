---
title: B: Judgment in Managerial Decision Making
---

- Author:: [[Max Bazerman]]

- Tags:: #[[Decision Making]] #Books #[[Problem Solving]]

- Status:: #reading #[[üì•inbox]]

- Takeways:
	 - Does knowing about cognitive biases help us avoid us those biases. #[[confirmation bias]]
id:: 2c7ded15-108d-4e1c-a689-825a7f6cfa9d
		 - Knowing about cognitive biases is helpful to reason out why we make [[mistake]]s after the fact, but they are not useful tools in [[preventing]] mistakes or motivating us to change [[behavior]]s
			 - this knowledge does not create [[automatic behaviors]] to apply change in our decision making

		 - If I know I am overconfident, can this knowledge protect me from thinking and behaving with  overconfidence

		 - If we want to [[solve]] the cognitive biases problem, it's not direct knowledge of them, but through multiple [[frame of reference]]s and [[viewpoint]]s. Take yourself out of the equation, walk through the problem through different [[stakeholder]]s and lenses

- Notes:
	 - We need to understand our thought processes and its effect on our behavior to influence how cognitive processes to lead us for successful decisions rather than leading us astray. 
		 - What are the best sources to study decision making and behavioral science?

		 - Are business practices considered the "best" practices truly proven to be so?

		 - Are business theories rigid and applicable to all situations or more flexible and needs to modified for each situation?

	 - Steps to [Decision Making]([[Decision Making]])
		 - ((6f4c492f-70be-4ed9-a3b2-bab89c064038))
			 - ((ee8ab90f-5841-4f61-97d0-9e4d3468a265))

			 - ((cd1a3e6f-5246-4ca9-91c2-8837a36dffbf))

			 - ((3ee5a84d-4655-46d1-a972-8b477b8c9f27))
				 - ((433a1fcc-e44c-488c-a616-b6f70ec6705b))

			 - ((25da5eed-1d79-49a2-98d7-779517d8c90d))
				 - ((1b2e473f-8341-411e-aeb6-e1ca8128dc89))

			 - ((baff4193-d2e5-4015-81ef-f4dd46e9886c))
				 - ((535e7ef4-21a7-4ce7-9865-b1b9081e850f))

			 - ((86f20d48-7cec-4614-9fc3-0b800da370a7))
				 - ((1a2cc53b-a665-418d-88d7-96bd4353eadf))

			 - ((77142d82-16ac-4d02-9d03-3c5f01d5d2dc))

			 - ((613bd3c4-0364-4549-8a91-031fc52bf42e))

		 - Refer to [[P: Applying mental models to decision making]] and improve the notes there #[[üì•inbox]]

		 - For each step, gain consensus with the decision maker

		 - [[Define the problem]] #[[Problem Solving]]
			 - Often we act on a problem without thoroughly [[understanding]] the issue, leading us to solve the wrong problem
				 - We are often too [overconfident]([[overconfidence]])to check double check and validate knowledge

				 - We don't remove our [[assumptions]] and [[biases]]

			 - We often make the following mistakes when:
				 - Defining the problems from an assumed solution #assumptions

				 - Missing the bigger problem
					 - What does "bigger" mean? Wrong scope? The problem is only a minor piece of a bigger problem and the solution will not solve the aggregate bigger problem

					 - Or find all the small problems to solve to create a difference 

					 - Defining the problem by its symptoms, we end up addressing the symptoms rather than the underlying root cause #[[root cause analysis]]

		 - Identify the criteria (from all individuals if a group problem)
id:: ff07511f-534a-4d64-9a9b-bbd700e6bfa7
			 - Identify all the objectives needed for problem to be considered "done"

			 - Example: When buying a car you may want to maximize fuel while minimizing cost.

		 - Weigh the criteria
id:: 0eda2439-949b-4bd7-90d4-834f179e7fb1
			 - Know the relative value of each criteria

			 - If making decisions in group, get a consensus on 

		 - Generate alternatives
			 - Identify all the possible courses of action

			 - An optimal search for solutions continue until the search outweigh the value of the added information

		 - Rate each alternative on each criterion
			 - How does each solution fulfill or achieve your criteria?

			 - Assess the potential consequences
				 - Employ [[Second-Order Thinking]] to find the potential outcomes

	 - [[System 1 and System 2 Thinking]]
id:: cae31e7b-b273-4925-a1b0-1f40ddb3f53e
		 - System 1 and 2 work in tandem
			 - Almost all thinking processes are a mix of both systems, and they are complementary
				 - Language: When we speak we are communicating consciously but are automatically unconsciously employing grammatical rules 

		 - Both systems can be biased and make mistakes
			 - Neither is categorically bad or good
				 - [[confirmation bias]] can affect both systems 1 and systems 2 thinking
					 - We may remember or notice information that supports our existing beliefs (system 1)

					 - We may be motivated to analyze new information that supports our existing beliefs or ignore discordant information (System 2)

			 - Good decision making relies on both

		 - System 1 thinking refers to our intuitive thinking, characterized as fast (instantaneous), automatic, effortless, implicit, and emotional.
			 - We make most decisions in life with System 1 thinking (emotional). 

			 - Examples: Interpretation of verbal language and visual information

			 - When we are [[busy]], [stressed]([[stress]]), or rushed we can inadvertently allow System 1 thinking to heavily influence or drown out System 2 thinking
				 - Our emotional state can influence our rational behavior

			 - When our schedule is too full or our minds is overwhelmed with too many thoughts or concerns we may find ourself defaulting to Systems 1 thinking
				 - [[E: We should stop defaulting to System 1 thinking and instead be intentional in our thinking]]

		 - [[System 2 thinking]] refers to reasoning that is slower, conscious effortful, explicit and logical. 

		 - More information, computation and time do not always result in better [[Decision Making]]

	 - The study of decision making
		 - [[Prescriptive Model of Decision Making]]
			 - Methods developed to help optimal decision making

			 - May employ mathematical model to help a decision maker act more rationally

			 - Optimal decision making however is dependent on the behaviors of others (rational or not)

			 - Plenty of research and advice on good decision making is available, but they do not account for how actual decisions are made by people
				 - Our [intuitions]([[intuition]]) may lead us astray and undermine our willingness to employ good advice

			 - We are limited by our lack of perfect knowledge

		 - [[Descriptive Model of Decision Making]]
			 - We learn how we make decisions (rationally or irrationally)

			 - Understanding our own decision making helps us clarify when and where we are likely to make mistakes and help us prepare for when better decision making strategies are needed

	 - [[Limitations of human attention]]
id:: 26ed890e-382d-4bd4-b420-acf1b13db941
		 - We can only retain a relatively small amount of information in our usable [[memory]], [[memory retention]]

		 - Variability in [[intelligence]] and perceptual acuity constrain our ability to accurately calculate the optimal choice from all available alternatives for the [[Prescriptive Model of Decision Making]] to work
			 - We typically overlook the full range of possible consequences and the best solution for one that is acceptable and reasonable enough. 

			 - Rather than examining and evaluating all possible alternatives we simply search until we find a satisfactory solution that wills suffice because it is good enough

	 - [[Judgmental Heuristics]]
		 - [[Heuristics]] or rule of thumbs are cognitive tools we use to simplify decision making

		 - [[Heuristics]] technique is an approach to a problem that employs a practical method that is not guaranteed to be optimal, perfect, or rational but is nevertheless sufficient for reaching an immediate, short-term goal or approximation.

		 - Example case: When looking to hire for an open marketing position and we employ a heuristic of limiting our candidates from the top 6 management schools that are new MBA graduates
			 - We will miss possibly the best qualified person due to this limitation

			 - By focusing on only the top six schools we save time and effort by employing this limited strategy
				 - This job heuristic could produce more good decisions than bad ones

				 - Economists would argue that the benefit of time saved often outweigh the cost of any potential reduction in the quality of the decision

				 - Evaluate your decisions by the identified criterion 

		 - Applications of heuristics creates problems due to our ignorance that we apply them in the first place

		 - Depending on [[Heuristics]] is beneficial when the loss in decision quality is outweighed by the time saved. #[[efficiency vs effectiveness]]

		 - The [[Availability Heuristic]]
			 - Q: People assess the frequency, probability, or likely cause of an event by the degree to which instances or occurrences of that event are readily "available" in [[memory]] ([[Tversky]] & [[Kahneman]], 1973) [[memory retention]]
				 - [Memories]([[memory]]) that evoke emotions are more vivid, easily imagined, and specific is more available than event that is unemotional in nature, bland, difficult to imagine or vague [[memory retention]]

			 - E: How do we employ our knowledge of the [[Availability Heuristic]] in our work environment decisions

		 - The [[Representative Heuristic]]
			 - When making judgment we often use past experiences and established stereotypes to guide the present evaluation
				 - Example: A manager may think white, ex-athletes perform best as salesperson, therefore future hiring evaluations are based on this criteria

			 - [[Representative Heuristic]] is often unconscious that leads to decision making that is discriminatory based on race, sex, or other traits that they would not engage in if they were conscious of the heuristic they were utilizing

			 - When we make¬†[decisions](https://www.verywellmind.com/why-you-make-bad-decisions-2795489)¬†based on representativeness, we may be likely to make more errors by overestimating the likelihood that something will occur. Just because an event or object is representative does not mean its occurrence is more probable.
id:: d04da4f3-f4d6-410e-aee9-8e60e6d028ef

		 - The [[Confirmation Heuristic]] 
			 - We intuitively use selective data when testing hypotheses that maybe faulty
				 - There is at minimum four distinct situations to consider when assessing the situation between two events, assuming each one hast just two possible outcomes. 
					 - For example: Is marijuana use related to delinquency
						 - We may only think of two groups. Marijuana users are delinquent and those who aren't when evaluating this statement

			 - Given the absence of evidence to the contrary (if just not explored sufficiently or due to imperfect knowledge), people tend to behave as if they assume that a given statement or hypothesis is true. 
				 - This tendency leads [[confirmation bias]]
					 - We search for and interpret evidence that supports our conclusions formed from the outset

		 - The [[Affect Heuristic]]
			 - q: Most of our judgments follow an unconscious, affective or emotional, evaluation that occurs even before any higher-level reasoning takes place ([[Khaneman]], 2003), rather than engage in complete analysis and reasoning process #[[Decision Making]]

	 - Chapter 2 - [[overconfidence]]
		 - If we were correctly humble about the quality of our judgments, we would be lead to easily double-check our opinions and correct our flaws. #validate 

		 - Differentiate between confidence vs overconfidence

		 - [Overconfidence]([[overconfidence]]) has been studied in three ways:
			 - [[overprecision]] describes the tendency to be too sure our judgments and decisions are accurate, uninterested in testing our [[assumptions]], and dismissive of evidence suggesting we might be wrong.
				 - Too certain we know the truth.

				 - The cause of overprecision is thought to be the desire to relieve internal dissonance regarding the right course of action or decision. 
					 - People in stressful situations are motivated to relieve this tension and willing to change what they believe 

					 - We prefer to seek out perspectives similar to our own even though a different perspective is more beneficial #[[alternative viewpoints]]

				 - Overprecision may also be caused by our cognitive tendency to seek memories that confirm and validate our beliefs than than those that disconfirm them

				 - [Confidence]([[confidence]]) is persuasive

				 - Outward expressions of [[confidence]] earn more [[trust]], [[credibility]], and [[status]]

				 - Overprecision makes us too sure of our [[judgments]] that we often do not [[doubt]] the fact we are often in error
					 - We are reluctant to seek out advice from others

					 - Suspicious of views that differs from our own
						 - We are too confident on our own opinions and place significantly less weight on others opinion---including useful [[advice]] #[[alternative viewpoints]] #help

					 - Too quick to act on our opinions

					 - Too slow to update our erroneous beliefs

			 - [[overestimate]] the tendency to think we're better across many domains than we actually are. 
				 - We [[overestimate]] how much we can accomplish in a limited amount of time or believe we can control more than we actually do.

				 - [[self-enhancement]]: People are motivated to view themselves positively rather accurately
					 - We tend to think the groups we associate with are better than others

				 - The [[illusion of control]]:  we think that we have more control over circumstances that we do
					 - This delusion is more pronounced when we have less control over something

					 - When we have more control, we actually underestimate our control

				 - The [[planning fallacy]]: overestimation of the speed at which we will complete projects and tasks 
					 - [[planning fallacy]] often occurs for large complex projects that are prone to complications

					 - [[Z: Agile methodology can prevent planning fallacy]]
						 - Can using [[agile]] methodology help us prevent [planning fallacies]([[planning fallacy]]) from occurring?

				 - [[optimistic biases]]
					 - The tendency to overestimate the promising prospects of our future, known as an unrealistic optimism.

					 - This type of optimism is not universal

			 - [[overplacement]]  the tendency to think we rank higher than others on certain dimensions, particularly in competitive contexts. 
				 - Over-placement can cause us to have unrealistic and inflated expectations
					 - This can lead us to hold on to failing projects longer than necessary

				 - We tend to over-place ourselves for easy tasks and project and under-place for complicated projects/tasks

				 - We often focus on ourselves and exaggerate our abilities and limitations and we fail to consider that others face similar opportunities and challenges. 

			 - Interventions that force us to think about alternative perspectives, interpretations, or hypothesis are often effective in combating overconfidence. #validate #[[alternative viewpoints]]

			 - Overconfidence is an unconscious process

		 - Negative Effects of [[overconfidence]]
			 - Positive illusions of ourselves leads us to claim inappropriate proportion of credit for positive outcomes
				 - We overestimate our value in relation to our organization 

				 - We set objectives that have little chance of success

			 - We can defer negative outcomes to other people and attributing bad luck when judging ourselves #[[Fundamental Attribution Error]]
				 - [[Z: We miss learning opportunities in our failures when we don't take ownership of them]]

		 - We can maintain an unrealistic view of ourselves (overconfidence) when we lack the opportunity or data to discomfirm those beliefs.
			 - In comparison when reality and sufficient information is present to provide feedback on our expertise or lack thereof, we are more likely to underestimate ourselves
				 - [[Our underestimation of ourselves causes us to forgo opportunities]] at which we would succeed if we only had the [[courage]] to try #permanentNotes

		 - We should strive to well informed during the decision making process. #[[Decision Making]] #[[P: Applying mental models to decision making]]

	 - Chapter 3 - Common Biases #biases
		 - Biases results from the inappropriate application of a heuristic in decision making or judgments.

		 - We mistakenly rely on our institution for assessments of our decision making process because we lack objective feedback on the quality of our decisions  #[[Decision Making]]
			 - Our reliance [[intuition]] can lead us to misapply a decision making process that we deemed successful from our past experience to another problem with a completely different context in the future

		 - [[Ease of Recall]] Bias
			 - This bias arise from the [[Availability Heuristic]] based on the vividness and recency of information

			 - We tend to pay more attention to more vivid, readily or easily recalled information that eclipses more relevant (bland) information needed to make an informed decision

			 - More memorable events are recalled easier than commonplace events 

		 - [[Retrievability]] Bias #[[Availability Heuristic]]
			 - Facts and memories are easier to retrieve will overpower information that requires greater effort to remember

		 - [Insensitivity to Base Rates]([[insensitivity to base rates]])
			 - Based on [[Representative Heuristic]]

			 - Ignoring statistical information in favor of using irrelevant information, that one incorrectly believes to be relevant, to make a judgment. This usually stems from the irrational belief that statistics don't apply to a situation, for one reason or another when, in fact, they do

			 - Don't let relevant information cloud your judgment away from learning from history or from statistical facts.

			 - Related sites: [Farnam Street](https://fs.blog/2012/11/mental-model-bias-from-insensitivity-to-base-rates/)

		 - [Insensitivity to Sample Size]([[insensitivity to sample size]])
			 - Based on [[Representative Heuristic]]
				 - ((d04da4f3-f4d6-410e-aee9-8e60e6d028ef))

			 - We intuitively know that the "results of larger samples deserve more trust than smaller samples, and even people who are innocent of statistical knowledge have heard about this law of large numbers."

			 - The principle of regression to the mean states that as the sample size grows the results should converge a stable frequency.

			 - In our minds however we fail to account for the accuracy and uncertainty of a given large or small sample size.

			 - Related link: [Farnam Street](https://fs.blog/2013/05/mental-model-bias-from-insensitivity-to-sample-size/)

		 - [[Regression to the Mean]] #[[Representative Heuristic]]
			 - In series of events that are dependent on many variables involving chance, extreme outcomes tend to be followed by moderate ones trending towards the mean

			 - This thinking can be counterintutitive because of representative heuristics.
				 - We tend to assume that future outcomes will be directly predictable by today's or from past outcomes

		 - The [[Fallacy of Conjunction]] #[[Representative Heuristic]]
			 - A conjunction (a combination of two ore more descriptors) cannot be more probably than any one of its descriptors. 

			 - The conjunction fallacy can also be triggered if the conjunction is more vivid and memorable than any of its descriptors

		 - The [[Confirmation Trap]] #[[confirmation bias]] #disconfirmation
			 - We naturally seek information that confirms our thinking instead of prioritizing disconfirmation or falsifying information even when going against the seeming trend is more beneficial or helpful

			 - When we encounter information that agrees with our established thinking we are less scrutinizing of its validity than if we encountered discordant knowledge
				 - We tend to favor information that that leads to conclusions we subconsciously favor 

				 - We process information in a biased manner, preferring knowledge that is consistent to our prior beliefs 

			 - The search for disconfirming evidence will provide the most useful insights
id:: 306836e8-b06e-46a0-8434-0335604197b6

		 - [[Anchor Bias]]
			 - Our decisions can be affected by anchors that is irrelevant to the context of our decisions making
				 - Numerical anchors can affect our estimation

				 - First impressions can alter our behavior and perceptions 

				 - A person's race and ethnicity can influence our behavior and expectations for that person 

		 - [[Conjunctive Bias]] and [[Disjunctive Bias]]
			 - Overestimation of conjunctive events offers a power explanation for the overestimation of future productivity and the [[planning fallacy]].

			 - Disjunctive events leads us to expect the worst. 
				 - Our awareness of our underestimation of disjunctive events sometimes makes us too pessimistic. 

		 - [[Hindsight Bias]]
			 - We tend to overestimate what we knew beforehand based upon what we later learned. 

			 - Hindsight bias often occurs when people look back on their own decisions and judgments and those of others

			 - The knowledge of an event's outcome acts as [[Anchor Bias]] to our interpretation of our prior judgments of the event's likelikhood. 

			 - Hindsight knowledge biases our perception of what we remember knowing in foresight. 

			 - Historical knowledge in support with the actual outcome may become easier to remember due to it being more cognitively [[salient]]

			 - Hindsight makes it difficult to learn from our past mistakes and evaluate our decision making processes objectively #[[P: Keep a journal chronicling your decisions and evaluate their outcomes based on what you were trying to achieve]]
id:: 5dbb51f1-1795-48e6-8fc7-693152238d47
				 - We make worse evaluations of our past decisions because hindsight colors our evaluation and logic
id:: 57c15e5e-ade7-4ebe-8a05-a9246c1220ee

			 - [[Curse of knowledge]]
				 - When we evaluate and access other's knowledge, we are unable to to ignore knowledge that they have that others do not
					 - Available knowledge is hard to forget when we try to think how much other have knowledge on a subject, which makes it impossible to have a fair evaluation of someone

				 - We have to be clear, specific, and distinct when giving out directions or [[summarizing]] #communication #[[Unclear [[writing]] ]]

				 - A potential remedy to the [[Curse of knowledge]] is to perceive the differences in people or objects as opposed to similarities

			 - To improve our quality of decisions, we have to overcome [[Confirmation Heuristic]] and consider a range of alternative explanations
				 - When presented with multiple pieces of information, we are good at picking out useful information, rather than information that confirms their expectations

	 - Chapter 4 - [[Bounded Awareness]]
		 - People have bounded awareness that often leads people to ignore accessible, perceivable, and important information while paying attention to other equally accessible but irrelevant information. 

		 - We are constantly engaged in information filtering to avoid problems associated with information overload.
			 - Our brains are not capable of paying to every information that may be potentially relevant to our decision making 

			 - Even if we were capable of paying attention to every piece of information presented to us, we still need a system to prioritize the importance of the information

		 - Sometimes we can create artificial bounds that don't exist to confirm to our previous understanding.  (page 64)

		 - [[Intentional Blindness]]
			 - When we intently narrow our focus on a specific task or problem, we may become blind to relevant and important information happening around our peripheral vision

		 - [[Change Blindness]]
			 - A slow change in information may not be noticeable
				 - For example we may not notice a slip in ethical/moral state of a company if they slowly slide into dysregulation and rule breaking compared to if there was an acute change in behavior

		 - [[Focalism]] and the Focusing Illusion
			 - Our tendency to focus too much on a particular event and too little on other events that are likely to occur concurrently. 
				 - We can overweight and influence our emotions and decisions based on an event. 
					 - For example our favorite sports team or politician winning or losing.

			 - [[Focusing Illusion]]
				 - Our tendency to only focus on a subset available information and overweighing their importance and underweight information we are not aware of. 

		 - [[Group [[Bounded Awareness]]]]
			 - Groups tend to pay more attention to knowledge the whole group already knows rather than focusing on unique information one individual may possess
				 - Groups discuss more shared information than unshared information
					 - [[Before group decision making, make sure information is shared beforehand]]

					 - Allow shared information to circulate and even force a pre-discussion

			 - Highlight the unique area of expertise individuals may have

			 - If you are solving a decision with [[Group [[Bounded Awareness]]]] think of the choices the other parties may make

			 - When [[negotiating]] talk to the other party to reduce information disparity

		 - [[Winner's curse]]
			 - The uneasy feeling after a purchase, especially when you don't know the true value of the item you bought. 

			 - The key feature of the "winner's curse" is that one side often has much better [[information]] than the other side. #[[Decision Making]]

		 - A remedy to lessen [[Bounded Awareness]] is to know the rules of the negotiation/decision and the perspective of the other players
			 - Be the most informed

		 - [[Reference group neglect]]
			 - [[Bounded Awareness]] also affects our assessments of our competitors, where we are insensitive to the quality of our competition

			 - We are too focused on ourselves, our strengths and weaknesses than on the competition
				 - This manifests in the majority of people entering contests that are easy, while ignoring and too fearful of contests that are more difficult or requires a specific skill

			 - Reference group neglect also can pertain to our own knowledge domains, and not concentrating on domains we don't know
				 - For example though we may care about campaign finance reform, when pit in competition to other issues we are more knowledgeable of the campaign finance reform issues rank in our importance list due to the fact we are less aware of its effects

				 - We [[value]] issues that are more clearly seen as end states or outcomes like tax cuts or education rather than using a broader [[awareness]] that would direct our attention toward a set of outcomes that have a larger, positive on many issues.

		 - Understanding the bounds of others [[[[choice]] overload]]
			 - When providing [[options]] and [[choices]] for others, it is best to limit their choices #[[Decision Making]]
				 - When people are given too many options, they favor taking inaction instead, even if we have clear preferences over inferior options

		 - 

	 - Chapter 5 - [[Framing]] and the [[Reversal of [[Preferences]]]]
		 - We treat risks concerning perceived gains differently from risk concerning perceived loses
			 - When we make [[decisions]] regarding [[losses]] we are [[[[risk]] seeking]]
id:: 5e3ca707-d4fe-43e3-9001-235c67edb06a

			 - We are [[[[risk]] averse]] when making decisions about [[gains]]
id:: 52f24daf-9b8b-4105-b795-35090ed45a99

			 - Example:
				 - Imagine that the United States is preparing for the outbreak of an
unusual Asian disease that is expected to kill 600 people. Two alternative programs to combat the disease have been proposed. Assume that the exact scientiÔ¨Åc estimates of the consequences of the programs are as follows.
					 - Program A: If Program A is adopted, 200 people will be saved.

					 - Program B: If Program B is adopted, there is a one-third probability that 600 people will be saved and a two-thirds probability that no people will be saved.

					 - Program C: If Program C is adopted, 400 people will die.

					 - Program D: If Program D is adopted, there is a one-third probability that no one will die and a two-thirds probability that 600 people will die

				 - Between A and B we will choose A because it is a decision about gain.

				 - Between C and D we will choose D because it is a decision about loss. 

		 - When proposing options in a decision, keep in mind the concept of [[diminishing returns]].
			 - Example: 
				 - You can (a) receive $10 million for sure (expected value $10 million) or (b) Ô¨Çip a coin and receive $22 million for heads but nothing for tails (expected value ¬º $11 million). An expected-value decision rule would require you to pick (b). What would you do?

			 - Most people would choose a because there is more value in the first $10 million that would not be equal to getting another $12 million.

		 - [[Re[[framing]]]] #[[Decision Making]]
			 - Reframing answers or choices can cause us to switch our initial preferences to the technically the same question/choices/answers

			 - We can make suboptimal group decisions when we make sequential, interconnected decisions that may have different framings

			 - In group decisions, we have to be aware of the framing of our team members to promote the best decisions and actions. 
				 - Be aware that some team members maybe asked to think in a [[[[risk]] seeking]]fashion versus a [[[[risk]] averse]] mindset #[[loss averse]]

			 - Individuals and groups need to become aware of this bias and develop procedures for identifying and integrating [[[[risk]]y decisions]] across organizations to arrive at a coherent strategy for making judgments under [[uncertainty]]
				 - [[Expected Value]]
					 - A decision making tool that calculates the sum of all possible values of all possible outcomes

					 - Use expected value for most decisions, unless the decision is very important

					 - Example: Imagine you're playing a coin-flipping game with a friend, and you wager $1. If the coin comes up heads, you win $2, but if the coin comes up tails, you lose your original $1 investment. There's a 50 percent chance you'll gain $1 and a 50 percent chance you'll lose $1. The expected value of the scenario, then, is $0, making it a neutral investment.

Let‚Äôs say you get $3 if heads comes up, however. In that case, you‚Äôll have a 50 percent chance of gaining $2 and a 50 percent chance of losing $1, so the EV of your scenario is $1, making it worth the risk, on average.

			 - [[Cost Benefit Analysis]]
				 - Cost benefit analysis doesn't work well in risky decisions because of our inherent biases of risk aversion/risk seeking depending on the framing of the choices and answers

			 - [[Certainty]] and [[Pseudocertainty]]
				 - We favor and value any actions that reduces the probability of harm, to say 0.1 to zero, compared to .2 to 0.1 even though the risk reduction is equal.

				 - Pseudocertainty is when risks or harm is reduced to zero

			 - [[Acquisition utility]]
				 - The  value you place on a commodity

			 - [[Transactional utility ]]
				 - The quality of the deal that you receive, evaluated in reference to "what the item should cost"

				 - Example: We may begrudgingly overpay for alcohol from a hotel resort compared to not buying the same alcohol drink in a grocery. We rationalize that groceries should not be ripping us off.

			 - The [[value]] we place on what we own
				 - [[endowment effect]]
					 - [[Emotional bias]] that causes individuals to value an [[owned]] object higher, often [[irrational]]ly, than its market value

				 - If given a $10 certificate or coupon, we will spend more money 

				 - We value the first [[incremental]] [[gain]] or [[loss]] more. For example we will be more upset if we lost $100 twice in the same day rather than losing $200 all at once.
					 - Do not give [[gift]]s at one time, they will be [[value]]d more given at separate occasions  

				 - [[framing]] can set our [[expectation]]s on [[value]].
					 - If we are given money and call it a [[bonus]] we are more likely to spend than if the money was called a [[rebate]]

			 - [[Joint versus separate preference reversal]]s
				 - When we place higher [[value]] on one option than another when looking at them individually, but [[reverse]] our [[preference]] when considering two or more options at the same time.
					 - Because we can [[compare]] and contrast the differences between the two options side by side. 

					 - To [[manipulate]] people on value, try to [[separate]] options by time or only give one option to provide no basis for comparison
						 - When we [[evaluate]] options separately or given only one option we will value our [[want]]s more over what we [[should]] [[choose]]. When evaluating two or more options at one time then our more [[logical ]]and [[reason]]ed "should" option will be valued higher
							 - We act more on our [[affective]] [[preference]]s when [[assessing]] one option at a time, but [[joint assessment]]s trigger a more [[reason]]ed [[analysis]]

			 - When facing [[risk]]y [[decision]]s, identify your [[reference]] point, then consider whether other reference points exist and whether they are just as reasonable. Then [[think]] about your decision from [[multiple]] [[perspective]]s and [[examine]] any [[contradiction]]s that may appear. 

			 - Why is our brain susceptible to biases and manipulation?
				 - Our reliance on [[frames]] and [[reference point]]s to assess outcomes is a solution to our biological constraint of our [[subjective utility scale]], our ability to experience pleasure and pain, is not definitively infinite.
					 - We [[value]] more the initial amount gained or feel the pain of the initial amount lost, then subsequent gains and losses. Given the sensitivity of our [[subjective utility scale]] we need to readjust our [[frames]] to compensate, as to not feel that achieving more than we have will not bring us greater [[joy]] or a sense of [[achievement]].

	 - Chapter 6 - [[Motivational]] and [[emotional]] [[influence]]s on [[Decision Making]]
		 - [[utilitarianism]] thinking is often described as "doing the greatest good for the greatest number of people"

		 - [[deontological]] thinking judges morality of an action based on the action's adherence to the rules or duties

		 - our [[emotion]]s are not consistently applied in our [[Decision Making]] process
			 - For example in the [[footbridge dilemma]] we would not choose to push over a man to an oncoming train to save five people but with the [[trolley dilemma]] we are willing to switch the train to kill one person to save five people
				 - People have internal [[inconsistencies]] between transient concerns and long-term  [[self-interest]] reflect the tension between people want to do and what they think they should do

				 - the impact of temporal difference in decision making shows up as discounting
					 - Any choice that involves a trade off between current and future benefits should discount the future to some extent

					 - [[hyperbolic discount]]ing, relative to the present time period we view all gain and losses in the future to be worth less that they would be in the present, example enjoying junk food now is worth subjectively more to you today than it would be tomorrow or a year from now. 

			 - [[Reconciling]] [[internal [[conflict]]]]s
				 - For our long term health and safety , should we try to allow the "[[should]]" self to completely control our [[decision]]s?
					 - Or  does the ''[[want]]" self have something valuable to add to improve the [[decision]]s of the ''should" self?

					 - The key to [[resolving]] our [[internal [[conflict]]]]s is to create a means of [[control]]ling the [[destructive]] [[impulse]]s of the [[short-term]] decision maker. 
						 - The ''should" self is the planner, it can develop advance schemes to coral, co-opt, or control the "want" self. 
							 - For example when dieting the "should" self should find enjoyable exercise, and make sure that there is healthy food for the "want" self.

		 - Self-[[serving]] [[reasoning]]
			 - [[perception]]s and [[expectation]] are often [[biased]] in a self-serving manner. 
				 - when presented with identical information, individual s perceive situation in dramatically different ways, depending on their roles in the situation. 
					 - we assess outcomes and determine our preference based on [[self-interest]] and then justify our [[preference]]s on the basis of [[fairness]] by changing the importance of [[attribute]]s affecting or defining what is [[fair]]
						 - while our goal is to reach a [[fair]] [[solution]] we are often [[biased]] for our [[self-interest]]

						 - [[self-serving]] [[reasoning]] allows people believe that it is honestly [[fair]] for them to have more of a given [[resource]]

						 - [[intelligent]] well-intentioned people come to biases conclusions even as they continue to believe in their own [[fairness]]

				 - [[emotion]]s influence decision making
					 - [[disgust]] focuses our [[attention]] on physical [[contamination]] and motivates us to purge our bodies and our minds of contaminating agents

					 - [[fear]] makes our minds [[sensitive]] to [[risk]]s and prepares our bodies to [[flee]]

					 - [[sadness]] focuses attention to [[self]], leading people to [[ruminate]] more and [[motivating]] them to [[change]]

					 - [[anger]] shares many features with [[happiness]] including increased [[confidence]], feeling of [[power]], and decreased [[sensitivity]] to [[risk]]

					 - [[emotion]]s have a great affect on the [[endowment effect]]
						 - emotions can bleed from one [[context]] to another. [[disgust]] can make sellers more readily to sell at a lower price because they want to get rid of things
							 - [[sadness]] [[trigger]]red the goal to change one's circumstances and more willing to pay to buy and decreasing the price they demanded to sell

						 - [[sad]] people are more [[pessimistic]] and [[happy]] people are more [[optimistic]] in decision making

				 - [[mood-congruent]] recall
					 - [[depressed]] people report that they cannot remember when it felt like to be happy and [[happy]] people are less likely to remember difficult times
						 - it is best to ask for a [[raise]] when your [[supervisor]] is [[happy]]
							 - the supervisor is more likely to recall times when you performed well and feel more optimistic about the company's ability to afford a raise for you

					 - the [[weather]] can also affect our [[mood]]
						 - people are more happy with their lives when it is [[sunny]] outside than it is when [[cloudy]]

					 - we are better at [[Remembering]] information consistent with our state of mind than information inconsistent with it

				 - [[regret]] [[avoidance]]
					 - Anticipation of [[regret]] is another area where [[emotions]] [[drive]] [[behavior]]
						 - the feeling of almost attaining a goal is more impactful than if were were not close to it at all.
							 - example: silver medalists are less happy than bronze medalists

							 - the [[motivation]] to minimize the opportunity for regret can lead people to make decisions that are suboptimal with respect to actual outcomes
								 - example: people are less likely to switch doors in the [[Monty Hall Problem]] is the [[fear]] of [[regret]] if they switched to an empty door 

								 - decision makers will choices that shield them from regrets

			 - we often think that our [[emotion]]s are [[uncontrollable]]. Even if we can't stop ourselves from feeling, we may be able to [[limit]] the [[negative]] effects of our emotions on the [[quality]] of our [[decision]]s
				 - We can identify our emotions and their sources. [[labeling]] our emotions can be an effective way to reduce their strength. 

				 - negative emotions can sometimes be more effectively [[neutralize]]d by knowing their sources, this allows people to react to the stimulus with [[System 2 thinking]]'s more cognitive [[assessment]]s

				 - we can also make decision makers [[accountable]] for their choices. When we have to [[justify]] our [[choice]]s we are more systematic and less emotional 
					 - to hold ourselves accountable you can verbalize your rationale to your supervisor or write it down. [[Decision Journal]]
