title:: Goodhart's Law Isn't as Useful as You Might Think (highlights)
author:: [[Cedric Chin]]
full-title:: "Goodhart's Law Isn't as Useful as You Might Think"
media:: #articles
url:: https://commoncog.com/goodharts-law-not-useful/

- Highlights first synced by [[Readwise]] [[2023-03-17]]
	- Goodhart’s Law is a famous adage that goes “when a measure becomes a target, it ceases to be a good measure.” ([View Highlight](https://read.readwise.io/read/01gvre1txt6t01jet2w42h9agf))
	- When people are pressured to meet a target value there are three ways they can proceed:  
	  
	  1) They can work to improve the system  
	  2) They can distort the system  
	  3) Or they can distort the data ([View Highlight](https://read.readwise.io/read/01gvre6a5s91fnaf25sn3t27rc))
	- This list of possible responses to quantitative targets is attributed to Brian Joiner, who ‘came up with this list several years ago’ — likely in the 80s. I immediately glommed onto this list as a more useful formulation than Goodhart’s Law. Joiner’s list suggests a number of solutions:
	  
	  1.  Make it difficult to distort the system.
	  2.  Make it difficult to distort the data, *and*
	  3.  Give people the slack necessary to improve the system (a tricky thing to do, which we’ve [covered elsewhere](https://commoncog.com/process-improvement-is-hard/)).
	  
	  The third point is really important. Preventing distortions is just one half of the solution. Avoiding Goodhart’s Law requires you to *also* give people the space to improve the system. Which begs the question: how *do you encourage people to do just that?* ([View Highlight](https://read.readwise.io/read/01gvrefe9e4sf44cqbhb1wdwgd))
	- Before you can improve any system **you must listen to the voice of the system** (the *Voice of the Process*). Then you must **understand how the inputs affect the outputs of the system**. Finally, you **must be able to change the inputs (and possibly the system) in order to achieve the desired results**. This will require sustained effort, constancy of purpose, and an environment where continual improvement is the operating philosophy. ([View Highlight](https://read.readwise.io/read/01gvrek2jpzkq93tq63ccgevp4))
	- Comparing numbers to specifications will not lead to the improvement of the process. Specifications are the Voice of the Customer, not the Voice of the Process. **The specification approach does not reveal any insights into how the process works**.  
	  
	  So if you only compare the data to the specifications, then you will be unable to improve the system, and will therefore be left with only the last two ways of meeting your goal (i.e. distorting the system, or distorting the data). When a current value is compared to an arbitrary numerical target (... it) will always create a temptation to make the data look favourable. And distortion is always easier than working to improve the system. ([View Highlight](https://read.readwise.io/read/01gvremm277m6ch21macfc4ht3))
	- Voice of the Customer’ and ‘Voice of the Process’ are fancy ways to say something simple. A target, goal, or budget usually represents some kind of ‘specification’ — some form of demand from the customer or from company management. This is the so-called ‘Voice of the Customer’. The ‘Voice of the Process’, on the other hand, is *how the process actually works.* ([View Highlight](https://read.readwise.io/read/01gvrenynhhdhkm99maemnx706))
	- This is a naive view of process improvement, and while it may work for something as simple as weight control, it is *not* going to work for the kind of complex processes that you would find in a typical business.
	  
	  Why?
	  
	  Business processes are often processes where you don’t know the inputs to your desired output ([View Highlight](https://read.readwise.io/read/01gvreq8eeqt50dwrhrn71gewc))
	- Well, let’s think about the weight control example. Losing weight is a process with two well known inputs: calories in (what you eat), and calories out (what you burn through exercise). This means that the primary difficulty of hitting a weight loss goal is to figure out how your body responds to different types of exercise or different types of foods, and how these new habits might fit into your daily life (this assumes you’re disciplined enough to stick to those habits in the first place, which, well, you know). By contrast, business processes are often processes where you *don’t* know the inputs to your desired output. So the first step is to figure out what those inputs are, and *then* figure out what subset of those you can influence, and *then, finally*, figure out the causal relationships between the input metrics and output metrics. ([View Highlight](https://read.readwise.io/read/01gvrez7czhjjg8v0b1305jbe4))
	- This is a long winded way of saying that if you want to improve some process, *you have to ignore the goal first, in favour of examining the process itself*. On the face of it, this is common sense: you cannot expect your team to improve some metric if that metric isn’t directly controllable. ([View Highlight](https://read.readwise.io/read/01gvrezxthf536qm9j4qxq56vw))
	- You cannot improve a process by listening to the Voice of the Customer. You can only improve a process by listening to the Voice of the Process.” ([View Highlight](https://read.readwise.io/read/01gvrf4306cgne0n97pqf1e2j2))
	- The way that a WBR deck is constructed is instructive. Broadly speaking, Amazon divides its metrics into ‘controllable input metrics’ and ‘output metrics’. Output metrics are not typically discussed in detail, because there is no way of directly influencing them. (Yes, Amazon leaders understand that they are evaluated based on their output metrics, but they recognise these are lagging indicators and are not directly actionable). Instead, the majority of discussions during WBR meetings focus on *exceptions and trends in controllable input metrics*. In other words, a metrics owner is expected to explain abnormal variation or a worrying trend (slowing growth rate, say, or if a metric is lagging behind target) — and is expected to announce “nothing to see here” if the metric is within normal variance and on track to hit target. In the latter case, the entire room glances at the metric for a second, and then moves on to the next graph. ([View Highlight](https://read.readwise.io/read/01gvrfkpts49egjb46c34w372b))
	- How do you come up with the right set of controllable input metrics? The short answer is that you do so by trial and error. Let’s pretend that you want to influence ‘Marketing Qualified Leads’ (or MQLs) and you hypothesise that ‘percentage of newsletters sent that is promotional’, ‘number of webinars conducted per week’ and ‘number of YouTube videos produced’ are controllable input metrics that affect this particular output metric. You include these three metrics in your WBR metrics deck, and charge the various metrics owners to drive up those numbers. Over the period of a few months (and recall, the WBR is conducted every *week*) your leadership team will soon say things like “Hmm, we’ve been driving up promotional newsletters for awhile now but there doesn’t seem to be a big difference in MQLs; maybe we should stop doing that” or “Number of webinars seems pretty predictive of a bump in MQLs, but why is the bump in numbers this week so large? You say it’s because of the joint webinar we did with Tableau? Well, should we track ‘number of webinars executed with a partner’ as a new controllable input metric and see if we can drive that up?” ([View Highlight](https://read.readwise.io/read/01gvrfmh8cf1ta285qt86yb4qg))
- New highlights added [[2023-03-19]] at 11:43 PM
	- You’ll notice a pattern of trial and error with metrics in the points above, and this is an essential part of the process. The key is to persistently test and debate as you go ([View Highlight](https://read.readwise.io/read/01gvxrwp4pz3257xz5zhja9p1x))
	- Implicit in the WBR process is the understanding that the initial controllable input metrics you pick might be the wrong ones. As a result, the WBR acts as a *safety net* — a weekly checkpoint to examine the relationships between controllable input metrics (which are set up as targets for operational teams) and corresponding output metrics (which represent the fundamental business outcomes that Amazon desires). If the relationship is non-existent or negative, Amazon’s leadership knows to kill that particular input metric. Said differently, the WBR assumes that controllable input metrics are only important *if* they drive desirable outcomes — if the metric is wrong, or the metrics stops driving output metrics at some point in the future, the metric is simply dropped. ([View Highlight](https://read.readwise.io/read/01gvxrzyb06hysj3hkqde36h1a))
	- This is the *third* solution in Joiner’s list (“give people enough slack to improve the system so that they do so”). The WBR simply functions as a mechanism to let that happen ([View Highlight](https://read.readwise.io/read/01gvxs08anwg86j6ep3mge977f))
	- A larger point I want to make is that the WBR becomes a weekly *synchronisation mechanism* for the company’s entire leadership team. This is more important than it might first seem. Colin told me that the week-in, week-out cadence explains how Amazon’s leadership is eventually able to go through 400-500 metrics in a single hour. An external observer might be overwhelmed. But an insider who has been engaged in the WBR process over a period of months won’t see *500* different metrics — instead, their felt experience of a WBR deck is that they are looking at clusters of controllable input metrics that map to other clusters of output metrics. In other words, the WBR process forces them to *build a causal model of the business in their heads*. Repeated viewings of the same set of metrics will eventually turn into a ‘fingertip-feel’ of the business — execs will be able to say things like “this feels wrong, this dip is more than expected seasonal variation, what’s up?” — which can really only happen if you’re looking at numbers and trends every week. (This is why glancing at metrics are important, even when the metric owner announces “nothing to see here.”) Most importantly, though, the entire leadership team shares in the *same* causal model, since they would have been present for the laborious trial and error process to identify, control, and then manipulate each and every metric that mattered. ([View Highlight](https://read.readwise.io/read/01gvxs2z6knqgb4ncegy7yvc5d))
	- Incidentally, this also explains why there are *so many goddamn metrics* in a typical WBR deck. Any output metric of importance in a business will typically be influenced by multiple input metrics. Pretending otherwise is to deny the multi-faceted nature of business. This means that if you track 50 output metrics, the number of controllable input metrics you’ll need to examine will greatly exceed that number — and may change from week to week! However you cut it, your WBR deck will expand to become much larger than an outside observer might expect. This is why it is often a mistake to ‘present a small, simple set of metrics’ or to anchor on a single ‘North Star metric’ for a business. ([View Highlight](https://read.readwise.io/read/01gvxs3v2f082vv1tey22cgk3n))
	- The first is that the WBR is administered by a *completely autonomous group* — the Finance department. Each WBR meeting is opened by an individual from the Finance team, who takes note of all questions during the meeting and follows up with unfinished threads from the previous week’s WBR. Finance is responsible for certifying the data presented in the WBR; they sit in on the meeting, and speak up to correct misrepresentations (and also, I suspect, to serve as a physical reminder of their role). Elsewhere in the book Bryar and Carr note that Finance generates and monitors targets that are produced as part of the annual OP1 and OP2 planning process — the same targets that are overlayed onto WBR graphs. But the most important function that Finance plays in the WBR process is that they are empowered (and incentivised!) to investigate and dive deep into any of the metrics that are presented during the WBR. ([View Highlight](https://read.readwise.io/read/01gvxs6aph7fa6a7f3ev8rmwwp))
	- Jeff, Warren, and Tom all insisted that, regardless of whether the business was going well or poorly, the finance team should “have no skin in the game other than to call it like they see it,” based on what the data revealed ([View Highlight](https://read.readwise.io/read/01gvxs6w0jg05mff7g0qy5y981))
	- One often-overlooked piece of the puzzle is determining how to audit metrics. **Unless you have a regular process to independently validate the metric, assume that over time something will cause it to drift and skew the numbers**. If the metric is important, find out a way to do a separate measurement or gather customer anecdotes and see if the information trues up with the metric you’re looking at. So, a recent example would be testing for COVID-19 by region. It is not enough to look at the number of positive tests in your region as compared to another region with a population of a similar size. You must also look at the number of tests per capita performed in each region. Since both the number of positive tests and the number of tests per capita in each location will keep changing, you will need to keep updating your audit of the measurements. ([View Highlight](https://read.readwise.io/read/01gvxsa9hha0j0jfy1pm4tgvgh))